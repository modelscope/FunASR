[//]: # (<div align="left"><img src="docs/images/funasr_logo.jpg" width="400"/></div>)

(ç®€ä½“ä¸­æ–‡|[English](./README.md))

# FunASR: A Fundamental End-to-End Speech Recognition Toolkit

[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)


FunASRå¸Œæœ›åœ¨è¯­éŸ³è¯†åˆ«çš„å­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ä¹‹é—´æ¶èµ·ä¸€åº§æ¡¥æ¢ã€‚é€šè¿‡å‘å¸ƒå·¥ä¸šçº§è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜å¯ä»¥æ›´æ–¹ä¾¿åœ°è¿›è¡Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç ”ç©¶å’Œç”Ÿäº§ï¼Œå¹¶æ¨åŠ¨è¯­éŸ³è¯†åˆ«ç”Ÿæ€çš„å‘å±•ã€‚è®©è¯­éŸ³è¯†åˆ«æ›´æœ‰è¶£ï¼

<div align="center">  
<h4>
 <a href="#æ ¸å¿ƒåŠŸèƒ½"> æ ¸å¿ƒåŠŸèƒ½ </a>   
ï½œ<a href="#æœ€æ–°åŠ¨æ€"> æœ€æ–°åŠ¨æ€ </a>
ï½œ<a href="#å®‰è£…æ•™ç¨‹"> å®‰è£… </a>
ï½œ<a href="#å¿«é€Ÿå¼€å§‹"> å¿«é€Ÿå¼€å§‹ </a>
ï½œ<a href="https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README_zh.md"> æ•™ç¨‹æ–‡æ¡£ </a>
ï½œ<a href="#æ¨¡å‹ä»“åº“"> æ¨¡å‹ä»“åº“ </a>
ï½œ<a href="#æœåŠ¡éƒ¨ç½²"> æœåŠ¡éƒ¨ç½² </a>
ï½œ<a href="#è”ç³»æˆ‘ä»¬"> è”ç³»æˆ‘ä»¬ </a>
</h4>
</div>

<a name="æ ¸å¿ƒåŠŸèƒ½"></a>
## æ ¸å¿ƒåŠŸèƒ½
- FunASRæ˜¯ä¸€ä¸ªåŸºç¡€è¯­éŸ³è¯†åˆ«å·¥å…·åŒ…ï¼Œæä¾›å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆVADï¼‰ã€æ ‡ç‚¹æ¢å¤ã€è¯­è¨€æ¨¡å‹ã€è¯´è¯äººéªŒè¯ã€è¯´è¯äººåˆ†ç¦»å’Œå¤šäººå¯¹è¯è¯­éŸ³è¯†åˆ«ç­‰ã€‚FunASRæä¾›äº†ä¾¿æ·çš„è„šæœ¬å’Œæ•™ç¨‹ï¼Œæ”¯æŒé¢„è®­ç»ƒå¥½çš„æ¨¡å‹çš„æ¨ç†ä¸å¾®è°ƒã€‚
- æˆ‘ä»¬åœ¨[ModelScope](https://www.modelscope.cn/models?page=1&tasks=auto-speech-recognition)ä¸[huggingface](https://huggingface.co/FunASR)ä¸Šå‘å¸ƒäº†å¤§é‡å¼€æºæ•°æ®é›†æˆ–è€…æµ·é‡å·¥ä¸šæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æˆ‘ä»¬çš„[æ¨¡å‹ä»“åº“](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md)äº†è§£æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ã€‚ä»£è¡¨æ€§çš„[Paraformer](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)éè‡ªå›å½’ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«æ¨¡å‹å…·æœ‰é«˜ç²¾åº¦ã€é«˜æ•ˆç‡ã€ä¾¿æ·éƒ¨ç½²çš„ä¼˜ç‚¹ï¼Œæ”¯æŒå¿«é€Ÿæ„å»ºè¯­éŸ³è¯†åˆ«æœåŠ¡ï¼Œè¯¦ç»†ä¿¡æ¯å¯ä»¥é˜…è¯»([æœåŠ¡éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))ã€‚

<a name="æœ€æ–°åŠ¨æ€"></a>
## æœ€æ–°åŠ¨æ€
- 2024/05/15ï¼šæ–°å¢åŠ æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ï¼Œ[emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)ï¼Œ[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)ï¼Œ[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary)ï¼Œè¾“å‡ºæƒ…æ„Ÿç±»åˆ«ä¸ºï¼šç”Ÿæ°”/angryï¼Œå¼€å¿ƒ/happyï¼Œä¸­ç«‹/neutralï¼Œéš¾è¿‡/sadã€‚
- 2024/05/15: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 4.5ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 1.6ã€ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ 1.10 å‘å¸ƒï¼Œé€‚é…FunASR 1.0æ¨¡å‹ç»“æ„ï¼›è¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))
- 2024/03/05ï¼šæ–°å¢åŠ Qwen-Audioä¸Qwen-Audio-ChatéŸ³é¢‘æ–‡æœ¬æ¨¡æ€å¤§æ¨¡å‹ï¼Œåœ¨å¤šä¸ªéŸ³é¢‘é¢†åŸŸæµ‹è¯•æ¦œå•åˆ·æ¦œï¼Œä¸­æ”¯æŒè¯­éŸ³å¯¹è¯ï¼Œè¯¦ç»†ç”¨æ³•è§ [ç¤ºä¾‹](examples/industrial_data_pretraining/qwen_audio)ã€‚
- 2024/03/05ï¼šæ–°å¢åŠ Whisper-large-v3æ¨¡å‹æ”¯æŒï¼Œå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«/ç¿»è¯‘/è¯­ç§è¯†åˆ«ï¼Œæ”¯æŒä» [modelscope](examples/industrial_data_pretraining/whisper/demo.py)ä»“åº“ä¸‹è½½ï¼Œä¹Ÿæ”¯æŒä» [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py)ä»“åº“ä¸‹è½½æ¨¡å‹ã€‚
- 2024/03/05: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 4.4ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 1.5ã€ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ 1.9 å‘å¸ƒï¼Œdockeré•œåƒæ”¯æŒarm64å¹³å°ï¼Œå‡çº§modelscopeç‰ˆæœ¬ï¼›è¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))
- 2024/01/30ï¼šfunasr-1.0å‘å¸ƒï¼Œæ›´æ–°è¯´æ˜[æ–‡æ¡£](https://github.com/alibaba-damo-academy/FunASR/discussions/1319)
- 2024/01/30ï¼šæ–°å¢åŠ æƒ…æ„Ÿè¯†åˆ« [æ¨¡å‹é“¾æ¥](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary)ï¼ŒåŸå§‹æ¨¡å‹ [repo](https://github.com/ddlBoJack/emotion2vec).
- 2024/01/25: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 4.2ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 1.3ï¼Œä¼˜åŒ–vadæ•°æ®å¤„ç†æ–¹å¼ï¼Œå¤§å¹…é™ä½å³°å€¼å†…å­˜å ç”¨ï¼Œå†…å­˜æ³„æ¼ä¼˜åŒ–ï¼›ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ 1.7 å‘å¸ƒï¼Œå®¢æˆ·ç«¯ä¼˜åŒ–ï¼›è¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))
- 2024/01/09: funasrç¤¾åŒºè½¯ä»¶åŒ…windows 2.0ç‰ˆæœ¬å‘å¸ƒï¼Œæ”¯æŒè½¯ä»¶åŒ…ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™4.1ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™1.2ã€ä¸­æ–‡å®æ—¶å¬å†™æœåŠ¡1.6çš„æœ€æ–°åŠŸèƒ½ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([FunASRç¤¾åŒºè½¯ä»¶åŒ…windowsç‰ˆæœ¬](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2024/01/03: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 4.0 å‘å¸ƒï¼Œæ–°å¢æ”¯æŒ8kæ¨¡å‹ã€ä¼˜åŒ–æ—¶é—´æˆ³ä¸åŒ¹é…é—®é¢˜åŠå¢åŠ å¥å­çº§åˆ«æ—¶é—´æˆ³ã€ä¼˜åŒ–è‹±æ–‡å•è¯fstçƒ­è¯æ•ˆæœã€æ”¯æŒè‡ªåŠ¨åŒ–é…ç½®çº¿ç¨‹å‚æ•°ï¼ŒåŒæ—¶ä¿®å¤å·²çŸ¥çš„crashé—®é¢˜åŠå†…å­˜æ³„æ¼é—®é¢˜ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡cpuç‰ˆæœ¬))
- 2024/01/03: ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ 1.6 å‘å¸ƒï¼Œ2pass-offlineæ¨¡å¼æ”¯æŒNgramè¯­è¨€æ¨¡å‹è§£ç ã€wfstçƒ­è¯ï¼ŒåŒæ—¶ä¿®å¤å·²çŸ¥çš„crashé—®é¢˜åŠå†…å­˜æ³„æ¼é—®é¢˜ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡cpuç‰ˆæœ¬))
- 2024/01/03: è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 1.2 å‘å¸ƒï¼Œä¿®å¤å·²çŸ¥çš„crashé—®é¢˜åŠå†…å­˜æ³„æ¼é—®é¢˜ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡cpuç‰ˆæœ¬))
- 2023/12/04: funasrç¤¾åŒºè½¯ä»¶åŒ…windows 1.0ç‰ˆæœ¬å‘å¸ƒï¼Œæ”¯æŒä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™ã€ä¸­æ–‡å®æ—¶å¬å†™æœåŠ¡ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([FunASRç¤¾åŒºè½¯ä»¶åŒ…windowsç‰ˆæœ¬](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2023/11/08ï¼šä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡3.0 CPUç‰ˆæœ¬å‘å¸ƒï¼Œæ–°å¢æ ‡ç‚¹å¤§æ¨¡å‹ã€Ngramè¯­è¨€æ¨¡å‹ä¸wfstçƒ­è¯ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡cpuç‰ˆæœ¬))
- 2023/10/17: è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ä¸€é”®éƒ¨ç½²çš„CPUç‰ˆæœ¬å‘å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡cpuç‰ˆæœ¬))
- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€éŸ³è§†é¢‘è¯­æ–™åº“ï¼Œä¸»è¦æ˜¯åœ¨çº¿ä¼šè®®æˆ–è€…åœ¨çº¿è¯¾ç¨‹åœºæ™¯ï¼ŒåŒ…å«äº†å¤§é‡ä¸å‘è¨€äººè®²è¯å®æ—¶åŒæ­¥çš„å¹»ç¯ç‰‡ã€‚
- 2023.10.10: [Paraformer-long-Spk](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py)æ¨¡å‹å‘å¸ƒï¼Œæ”¯æŒåœ¨é•¿è¯­éŸ³è¯†åˆ«çš„åŸºç¡€ä¸Šè·å–æ¯å¥è¯çš„è¯´è¯äººæ ‡ç­¾ã€‚
- 2023.10.07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): FunCodecæä¾›å¼€æºæ¨¡å‹å’Œè®­ç»ƒå·¥å…·ï¼Œå¯ä»¥ç”¨äºéŸ³é¢‘ç¦»æ•£ç¼–ç ï¼Œä»¥åŠåŸºäºç¦»æ•£ç¼–ç çš„è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆç­‰ä»»åŠ¡ã€‚
- 2023.09.01: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡2.0 CPUç‰ˆæœ¬å‘å¸ƒï¼Œæ–°å¢ffmpegã€æ—¶é—´æˆ³ä¸çƒ­è¯æ¨¡å‹æ”¯æŒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡cpuç‰ˆæœ¬))
- 2023.08.07: ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ä¸€é”®éƒ¨ç½²çš„CPUç‰ˆæœ¬å‘å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡cpuç‰ˆæœ¬))
- 2023.07.17: BATä¸€ç§ä½å»¶è¿Ÿä½å†…å­˜æ¶ˆè€—çš„RNN-Tæ¨¡å‹å‘å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…ï¼ˆ[BAT](egs/aishell/bat)ï¼‰
- 2023.06.26: ASRU2023 å¤šé€šé“å¤šæ–¹ä¼šè®®è½¬å½•æŒ‘æˆ˜èµ›2.0å®Œæˆç«èµ›ç»“æœå…¬å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…ï¼ˆ[M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2_cn/index.html)ï¼‰

<a name="å®‰è£…æ•™ç¨‹"></a>
## å®‰è£…æ•™ç¨‹

```shell
pip3 install -U funasr
```
æˆ–è€…ä»æºä»£ç å®‰è£…
``` sh
git clone https://github.com/alibaba/FunASR.git && cd FunASR
pip3 install -e ./
```
å¦‚æœéœ€è¦ä½¿ç”¨å·¥ä¸šé¢„è®­ç»ƒæ¨¡å‹ï¼Œå®‰è£…modelscopeï¼ˆå¯é€‰ï¼‰

```shell
pip3 install -U modelscope
```

## æ¨¡å‹ä»“åº“

FunASRå¼€æºäº†å¤§é‡åœ¨å·¥ä¸šæ•°æ®ä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‚¨å¯ä»¥åœ¨[æ¨¡å‹è®¸å¯åè®®](./MODEL_LICENSE)ä¸‹è‡ªç”±ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹å’Œåˆ†äº«FunASRæ¨¡å‹ï¼Œä¸‹é¢åˆ—ä¸¾ä»£è¡¨æ€§çš„æ¨¡å‹ï¼Œæ›´å¤šæ¨¡å‹è¯·å‚è€ƒ [æ¨¡å‹ä»“åº“](./model_zoo)ã€‚

ï¼ˆæ³¨ï¼šâ­ è¡¨ç¤ºModelScopeæ¨¡å‹ä»“åº“ï¼ŒğŸ¤— è¡¨ç¤ºHuggingfaceæ¨¡å‹ä»“åº“ï¼ŒğŸ€è¡¨ç¤ºOpenAIæ¨¡å‹ä»“åº“ï¼‰


|                                                                                                     æ¨¡å‹åå­—                                                                                                      |        ä»»åŠ¡è¯¦æƒ…        |      è®­ç»ƒæ•°æ®      |  å‚æ•°é‡   | 
|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------:|:--------------:|:------:|
|    paraformer-zh <br> ([â­](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)  [ğŸ¤—](https://huggingface.co/funasr/paraformer-tp) )    |  è¯­éŸ³è¯†åˆ«ï¼Œå¸¦æ—¶é—´æˆ³è¾“å‡ºï¼Œéå®æ—¶   |   60000å°æ—¶ï¼Œä¸­æ–‡   |  220M  |
| paraformer-zh-streaming <br> ( [â­](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [ğŸ¤—](https://huggingface.co/funasr/paraformer-zh-streaming) ) |      è¯­éŸ³è¯†åˆ«ï¼Œå®æ—¶       |   60000å°æ—¶ï¼Œä¸­æ–‡   |  220M  |
|         paraformer-en <br> ( [â­](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [ğŸ¤—](https://huggingface.co/funasr/paraformer-en) )         |      è¯­éŸ³è¯†åˆ«ï¼Œéå®æ—¶      |   50000å°æ—¶ï¼Œè‹±æ–‡   |  220M  |
|                      conformer-en <br> ( [â­](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [ğŸ¤—](https://huggingface.co/funasr/conformer-en) )                      |      è¯­éŸ³è¯†åˆ«ï¼Œéå®æ—¶      |   50000å°æ—¶ï¼Œè‹±æ–‡   |  220M  |
|                        ct-punc <br> ( [â­](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [ğŸ¤—](https://huggingface.co/funasr/ct-punc) )                         |        æ ‡ç‚¹æ¢å¤        |   100Mï¼Œä¸­æ–‡ä¸è‹±æ–‡   |  1.1B  | 
|                            fsmn-vad <br> ( [â­](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [ğŸ¤—](https://huggingface.co/funasr/fsmn-vad) )                             |     è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼Œå®æ—¶      |  5000å°æ—¶ï¼Œä¸­æ–‡ä¸è‹±æ–‡  |  0.4M  | 
|                              fa-zh <br> ( [â­](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [ğŸ¤—](https://huggingface.co/funasr/fa-zh) )                               |      å­—çº§åˆ«æ—¶é—´æˆ³é¢„æµ‹      |   50000å°æ—¶ï¼Œä¸­æ–‡   |  38M   |
|                                 cam++ <br> ( [â­](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [ğŸ¤—](https://huggingface.co/funasr/campplus) )                                 |      è¯´è¯äººç¡®è®¤/åˆ†å‰²      |     5000å°æ—¶     |  7.2M  | 
|                                     Whisper-large-v3 <br> ([â­](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary)  [ğŸ€](https://github.com/openai/whisper) )                                      |  è¯­éŸ³è¯†åˆ«ï¼Œå¸¦æ—¶é—´æˆ³è¾“å‡ºï¼Œéå®æ—¶   |      å¤šè¯­è¨€       | 1550 M |
|                                         Qwen-Audio <br> ([â­](examples/industrial_data_pretraining/qwen_audio/demo.py)  [ğŸ¤—](https://huggingface.co/Qwen/Qwen-Audio) )                                         |  éŸ³é¢‘æ–‡æœ¬å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆé¢„è®­ç»ƒï¼‰   |      å¤šè¯­è¨€       |   8B   |
|                                 Qwen-Audio-Chat <br> ([â­](examples/industrial_data_pretraining/qwen_audio/demo_chat.py)  [ğŸ¤—](https://huggingface.co/Qwen/Qwen-Audio-Chat) )                                  | éŸ³é¢‘æ–‡æœ¬å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆchatç‰ˆæœ¬ï¼‰ |      å¤šè¯­è¨€       |   8B   |
|                        emotion2vec+large <br> ([â­](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)  [ğŸ¤—](https://huggingface.co/emotion2vec/emotion2vec_plus_large) )                        |    æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹          | 40000å°æ—¶ï¼Œ4ç§æƒ…æ„Ÿç±»åˆ« |  300M  |

<a name="å¿«é€Ÿå¼€å§‹"></a>
## å¿«é€Ÿå¼€å§‹

ä¸‹é¢ä¸ºå¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹ï¼Œæµ‹è¯•éŸ³é¢‘ï¼ˆ[ä¸­æ–‡](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav)ï¼Œ[è‹±æ–‡](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav)ï¼‰

### å¯æ‰§è¡Œå‘½ä»¤è¡Œ

```shell
funasr ++model=paraformer-zh ++vad_model="fsmn-vad" ++punc_model="ct-punc" ++input=asr_example_zh.wav
```

æ³¨ï¼šæ”¯æŒå•æ¡éŸ³é¢‘æ–‡ä»¶è¯†åˆ«ï¼Œä¹Ÿæ”¯æŒæ–‡ä»¶åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸ºkaldié£æ ¼wav.scpï¼š`wav_id   wav_path`

### éå®æ—¶è¯­éŸ³è¯†åˆ«
```python
from funasr import AutoModel
# paraformer-zh is a multi-functional asr model
# use vad, punc, spk or not as you need
model = AutoModel(model="paraformer-zh",  vad_model="fsmn-vad", punc_model="ct-punc", 
                  # spk_model="cam++"
                  )
res = model.generate(input=f"{model.model_path}/example/asr_example.wav", 
            batch_size_s=300, 
            hotword='é­”æ­')
print(res)
```
æ³¨ï¼š`hub`ï¼šè¡¨ç¤ºæ¨¡å‹ä»“åº“ï¼Œ`ms`ä¸ºé€‰æ‹©modelscopeä¸‹è½½ï¼Œ`hf`ä¸ºé€‰æ‹©huggingfaceä¸‹è½½ã€‚

### å®æ—¶è¯­éŸ³è¯†åˆ«

```python
from funasr import AutoModel

chunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms
encoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention
decoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention

model = AutoModel(model="paraformer-zh-streaming")

import soundfile
import os

wav_file = os.path.join(model.model_path, "example/asr_example.wav")
speech, sample_rate = soundfile.read(wav_file)
chunk_stride = chunk_size[1] * 960 # 600ms

cache = {}
total_chunk_num = int(len((speech)-1)/chunk_stride+1)
for i in range(total_chunk_num):
    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]
    is_final = i == total_chunk_num - 1
    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)
    print(res)
```

æ³¨ï¼š`chunk_size`ä¸ºæµå¼å»¶æ—¶é…ç½®ï¼Œ`[0,10,5]`è¡¨ç¤ºä¸Šå±å®æ—¶å‡ºå­—ç²’åº¦ä¸º`10*60=600ms`ï¼Œæœªæ¥ä¿¡æ¯ä¸º`5*60=300ms`ã€‚æ¯æ¬¡æ¨ç†è¾“å…¥ä¸º`600ms`ï¼ˆé‡‡æ ·ç‚¹æ•°ä¸º`16000*0.6=960`ï¼‰ï¼Œè¾“å‡ºä¸ºå¯¹åº”æ–‡å­—ï¼Œæœ€åä¸€ä¸ªè¯­éŸ³ç‰‡æ®µè¾“å…¥éœ€è¦è®¾ç½®`is_final=True`æ¥å¼ºåˆ¶è¾“å‡ºæœ€åä¸€ä¸ªå­—ã€‚

### è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆéå®æ—¶ï¼‰
```python
from funasr import AutoModel

model = AutoModel(model="fsmn-vad")

wav_file = f"{model.model_path}/example/vad_example.wav"
res = model.generate(input=wav_file)
print(res)
```
æ³¨ï¼šVADæ¨¡å‹è¾“å‡ºæ ¼å¼ä¸ºï¼š`[[beg1, end1], [beg2, end2], .., [begN, endN]]`ï¼Œå…¶ä¸­`begN/endN`è¡¨ç¤ºç¬¬`N`ä¸ªæœ‰æ•ˆéŸ³é¢‘ç‰‡æ®µçš„èµ·å§‹ç‚¹/ç»“æŸç‚¹ï¼Œ
å•ä½ä¸ºæ¯«ç§’ã€‚

### è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆå®æ—¶ï¼‰
```python
from funasr import AutoModel

chunk_size = 200 # ms
model = AutoModel(model="fsmn-vad")

import soundfile

wav_file = f"{model.model_path}/example/vad_example.wav"
speech, sample_rate = soundfile.read(wav_file)
chunk_stride = int(chunk_size * sample_rate / 1000)

cache = {}
total_chunk_num = int(len((speech)-1)/chunk_stride+1)
for i in range(total_chunk_num):
    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]
    is_final = i == total_chunk_num - 1
    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)
    if len(res[0]["value"]):
        print(res)
```
æ³¨ï¼šæµå¼VADæ¨¡å‹è¾“å‡ºæ ¼å¼ä¸º4ç§æƒ…å†µï¼š
- `[[beg1, end1], [beg2, end2], .., [begN, endN]]`ï¼šåŒä¸Šç¦»çº¿VADè¾“å‡ºç»“æœã€‚
- `[[beg, -1]]`ï¼šè¡¨ç¤ºåªæ£€æµ‹åˆ°èµ·å§‹ç‚¹ã€‚
- `[[-1, end]]`ï¼šè¡¨ç¤ºåªæ£€æµ‹åˆ°ç»“æŸç‚¹ã€‚
- `[]`ï¼šè¡¨ç¤ºæ—¢æ²¡æœ‰æ£€æµ‹åˆ°èµ·å§‹ç‚¹ï¼Œä¹Ÿæ²¡æœ‰æ£€æµ‹åˆ°ç»“æŸç‚¹
è¾“å‡ºç»“æœå•ä½ä¸ºæ¯«ç§’ï¼Œä»èµ·å§‹ç‚¹å¼€å§‹çš„ç»å¯¹æ—¶é—´ã€‚

### æ ‡ç‚¹æ¢å¤
```python
from funasr import AutoModel

model = AutoModel(model="ct-punc")

res = model.generate(input="é‚£ä»Šå¤©çš„ä¼šå°±åˆ°è¿™é‡Œå§ happy new year æ˜å¹´è§")
print(res)
```

### æ—¶é—´æˆ³é¢„æµ‹
```python
from funasr import AutoModel

model = AutoModel(model="fa-zh")

wav_file = f"{model.model_path}/example/asr_example.wav"
text_file = f"{model.model_path}/example/text.txt"
res = model.generate(input=(wav_file, text_file), data_type=("sound", "text"))
print(res)
```
æ›´è¯¦ç»†ï¼ˆ[æ•™ç¨‹æ–‡æ¡£](docs/tutorial/README_zh.md)ï¼‰ï¼Œ
æ›´å¤šï¼ˆ[æ¨¡å‹ç¤ºä¾‹](https://github.com/alibaba-damo-academy/FunASR/tree/main/examples/industrial_data_pretraining)ï¼‰

## å¯¼å‡ºONNX
### ä»å‘½ä»¤è¡Œå¯¼å‡º
```shell
funasr-export ++model=paraformer ++quantize=false
```

### ä»Pythonå¯¼å‡º
```python
from funasr import AutoModel

model = AutoModel(model="paraformer")

res = model.export(quantize=False)
```

### æµ‹è¯•ONNX
```python
# pip3 install -U funasr-onnx
from funasr_onnx import Paraformer
model_dir = "damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch"
model = Paraformer(model_dir, batch_size=1, quantize=True)

wav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']

result = model(wav_path)
print(result)
```

æ›´å¤šä¾‹å­è¯·å‚è€ƒ [æ ·ä¾‹](runtime/python/onnxruntime)

<a name="æœåŠ¡éƒ¨ç½²"></a>
## æœåŠ¡éƒ¨ç½²
FunASRæ”¯æŒé¢„è®­ç»ƒæˆ–è€…è¿›ä¸€æ­¥å¾®è°ƒçš„æ¨¡å‹è¿›è¡ŒæœåŠ¡éƒ¨ç½²ã€‚ç›®å‰æ”¯æŒä»¥ä¸‹å‡ ç§æœåŠ¡éƒ¨ç½²ï¼š

- ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ï¼ˆCPUç‰ˆæœ¬ï¼‰ï¼Œå·²å®Œæˆ
- ä¸­æ–‡æµå¼è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼ˆCPUç‰ˆæœ¬ï¼‰ï¼Œå·²å®Œæˆ
- è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ï¼ˆCPUç‰ˆæœ¬ï¼‰ï¼Œå·²å®Œæˆ
- ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ï¼ˆGPUç‰ˆæœ¬ï¼‰ï¼Œè¿›è¡Œä¸­
- æ›´å¤šæ”¯æŒä¸­

è¯¦ç»†ä¿¡æ¯å¯ä»¥å‚é˜…([æœåŠ¡éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))ã€‚


<a name="ç¤¾åŒºäº¤æµ"></a>
## è”ç³»æˆ‘ä»¬

å¦‚æœæ‚¨åœ¨ä½¿ç”¨ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥åœ¨githubé¡µé¢æIssuesã€‚æ¬¢è¿è¯­éŸ³å…´è¶£çˆ±å¥½è€…æ‰«æä»¥ä¸‹çš„é’‰é’‰ç¾¤æˆ–è€…å¾®ä¿¡ç¾¤äºŒç»´ç åŠ å…¥ç¤¾åŒºç¾¤ï¼Œè¿›è¡Œäº¤æµå’Œè®¨è®ºã€‚

|                                  é’‰é’‰ç¾¤                                  |                          å¾®ä¿¡                           |
|:---------------------------------------------------------------------:|:-----------------------------------------------------:|
| <div align="left"><img src="docs/images/dingding.jpg" width="250"/>   | <img src="docs/images/wechat.png" width="215"/></div> |

## ç¤¾åŒºè´¡çŒ®è€…

| <div align="left"><img src="docs/images/alibaba.png" width="260"/> | <div align="left"><img src="docs/images/nwpu.png" width="260"/> | <img src="docs/images/China_Telecom.png" width="200"/> </div>  | <img src="docs/images/RapidAI.png" width="200"/> </div> | <img src="docs/images/aihealthx.png" width="200"/> </div> | <img src="docs/images/XVERSE.png" width="250"/> </div> |
|:------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------:|:-------------------------------------------------------:|:-----------------------------------------------------------:|:------------------------------------------------------:|

è´¡çŒ®è€…åå•è¯·å‚è€ƒï¼ˆ[è‡´è°¢åå•](./Acknowledge.md)ï¼‰


## è®¸å¯åè®®
é¡¹ç›®éµå¾ª[The MIT License](https://opensource.org/licenses/MIT)å¼€æºåè®®ï¼Œæ¨¡å‹è®¸å¯åè®®è¯·å‚è€ƒï¼ˆ[æ¨¡å‹åè®®](./MODEL_LICENSE)ï¼‰


## è®ºæ–‡å¼•ç”¨

``` bibtex
@inproceedings{gao2023funasr,
  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},
  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},
  year={2023},
  booktitle={INTERSPEECH},
}
@inproceedings{An2023bat,
  author={Keyu An and Xian Shi and Shiliang Zhang},
  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},
  year={2023},
  booktitle={INTERSPEECH},
}
@inproceedings{gao22b_interspeech,
  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},
  title={{Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={2063--2067},
  doi={10.21437/Interspeech.2022-9996}
}
@article{shi2023seaco,
  author={Xian Shi and Yexin Yang and Zerui Li and Yanni Chen and Zhifu Gao and Shiliang Zhang},
  title={{SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability}},
  year=2023,
  journal={arXiv preprint arXiv:2308.03266(accepted by ICASSP2024)},
}
```
