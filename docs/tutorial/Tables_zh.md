# FunASR-1.x.xÂ æ³¨å†Œæ¨¡å‹æ•™ç¨‹

1.0ç‰ˆæœ¬çš„è®¾è®¡åˆè¡·æ˜¯ã€**è®©æ¨¡å‹é›†æˆæ›´ç®€å•**ã€‘ï¼Œæ ¸å¿ƒfeatureä¸ºæ³¨å†Œè¡¨ä¸AutoModelï¼š

*   æ³¨å†Œè¡¨çš„å¼•å…¥ï¼Œä½¿å¾—å¼€å‘ä¸­å¯ä»¥ç”¨æ­ç§¯æœ¨çš„æ–¹å¼æ¥å…¥æ¨¡å‹ï¼Œå…¼å®¹å¤šç§taskï¼›
    
*   æ–°è®¾è®¡çš„AutoModelæ¥å£ï¼Œç»Ÿä¸€modelscopeã€huggingfaceä¸funasræ¨ç†ä¸è®­ç»ƒæ¥å£ï¼Œæ”¯æŒè‡ªç”±é€‰æ‹©ä¸‹è½½ä»“åº“ï¼›
    
*   æ”¯æŒæ¨¡å‹å¯¼å‡ºï¼Œdemoçº§åˆ«æœåŠ¡éƒ¨ç½²ï¼Œä»¥åŠå·¥ä¸šçº§åˆ«å¤šå¹¶å‘æœåŠ¡éƒ¨ç½²ï¼›
    
*   ç»Ÿä¸€å­¦æœ¯ä¸å·¥ä¸šæ¨¡å‹æ¨ç†è®­ç»ƒè„šæœ¬ï¼›
    

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/a/6Ea1DxkZVte8y0g2/150e0eafd1c34f2dbb9360ccb5db4dc40521.png)

# å¿«é€Ÿä¸Šæ‰‹

## åŸºäºautomodelç”¨æ³•

### Paraformeræ¨¡å‹

è¾“å…¥ä»»æ„æ—¶é•¿è¯­éŸ³ï¼Œè¾“å‡ºä¸ºè¯­éŸ³å†…å®¹å¯¹åº”æ–‡å­—ï¼Œæ–‡å­—å…·æœ‰æ ‡ç‚¹æ–­å¥ï¼Œå­—çº§åˆ«æ—¶é—´æˆ³ï¼Œä»¥åŠè¯´è¯äººèº«ä»½ã€‚

```python
from funasr import AutoModel

model = AutoModel(model="paraformer-zh",  
                  vad_model="fsmn-vad", 
                  vad_kwargs={"max_single_segment_time": 60000},
                  punc_model="ct-punc", 
                  # spk_model="cam++"
                  )
wav_file = f"{model.model_path}/example/asr_example.wav"
res = model.generate(input=wav_file, batch_size_s=300, batch_size_threshold_s=60, hotword='é­”æ­')
print(res)
```

### SenseVoiceSmallæ¨¡å‹

è¾“å…¥ä»»æ„æ—¶é•¿è¯­éŸ³ï¼Œè¾“å‡ºä¸ºè¯­éŸ³å†…å®¹å¯¹åº”æ–‡å­—ï¼Œæ–‡å­—å…·æœ‰æ ‡ç‚¹æ–­å¥ï¼Œæ”¯æŒä¸­è‹±æ—¥ç²¤éŸ©5ä¸­è¯­è¨€ã€‚ã€å­—çº§åˆ«æ—¶é—´æˆ³ï¼Œä»¥åŠè¯´è¯äººèº«ä»½ã€‘åç»­ä¼šæ”¯æŒã€‚

```python
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model = AutoModel(
    model="iic/SenseVoiceSmall",
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
)

res = model.generate(
    input=f"{model.model_path}/example/en.mp3",
    language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
    use_itn=True,
    batch_size_s=60,
)
text = rich_transcription_postprocess(res[0]["text"])
print(text) #ğŸ‘Senior staff, Priipal Doris Jackson, Wakefield faculty, and, of course, my fellow classmates.I am honored to have been chosen to speak before my classmates, as well as the students across America today.
```

## APIæ–‡æ¡£

#### AutoModelÂ å®šä¹‰

```plaintext
model = AutoModel(model=[str], device=[str], ncpu=[int], output_dir=[str], batch_size=[int], hub=[str], **kwargs)
```

*   `model`(str):Â [æ¨¡å‹ä»“åº“](https://github.com/alibaba-damo-academy/FunASR/tree/main/model_zoo)Â ä¸­çš„æ¨¡å‹åç§°ï¼Œæˆ–æœ¬åœ°ç£ç›˜ä¸­çš„æ¨¡å‹è·¯å¾„
    
*   `device`(str):Â `cuda:0`ï¼ˆé»˜è®¤gpu0ï¼‰ï¼Œä½¿ç”¨Â GPUÂ è¿›è¡Œæ¨ç†ï¼ŒæŒ‡å®šã€‚å¦‚æœä¸º`cpu`ï¼Œåˆ™ä½¿ç”¨Â CPUÂ è¿›è¡Œæ¨ç†
    
*   `ncpu`(int):Â `4`Â ï¼ˆé»˜è®¤ï¼‰ï¼Œè®¾ç½®ç”¨äºÂ CPUÂ å†…éƒ¨æ“ä½œå¹¶è¡Œæ€§çš„çº¿ç¨‹æ•°
    
*   `output_dir`(str):Â `None`Â ï¼ˆé»˜è®¤ï¼‰ï¼Œå¦‚æœè®¾ç½®ï¼Œè¾“å‡ºç»“æœçš„è¾“å‡ºè·¯å¾„
    
*   `batch_size`(int):Â `1`Â ï¼ˆé»˜è®¤ï¼‰ï¼Œè§£ç æ—¶çš„æ‰¹å¤„ç†ï¼Œæ ·æœ¬ä¸ªæ•°
    
*   `hub`(str)ï¼š`ms`ï¼ˆé»˜è®¤ï¼‰ï¼Œä»modelscopeä¸‹è½½æ¨¡å‹ã€‚å¦‚æœä¸º`hf`ï¼Œä»huggingfaceä¸‹è½½æ¨¡å‹ã€‚
    
*   `**kwargs`(dict):Â æ‰€æœ‰åœ¨`config.yaml`ä¸­å‚æ•°ï¼Œå‡å¯ä»¥ç›´æ¥åœ¨æ­¤å¤„æŒ‡å®šï¼Œä¾‹å¦‚ï¼Œvadæ¨¡å‹ä¸­æœ€å¤§åˆ‡å‰²é•¿åº¦Â `max_single_segment_time=6000`Â ï¼ˆæ¯«ç§’ï¼‰ã€‚
    

#### AutoModelÂ æ¨ç†

```plaintext
res = model.generate(input=[str], output_dir=[str])
```

*   *   wavæ–‡ä»¶è·¯å¾„,Â ä¾‹å¦‚:Â asr\_example.wav
        
    *   pcmæ–‡ä»¶è·¯å¾„,Â ä¾‹å¦‚:Â asr\_example.pcmï¼Œæ­¤æ—¶éœ€è¦æŒ‡å®šéŸ³é¢‘é‡‡æ ·ç‡fsï¼ˆé»˜è®¤ä¸º16000ï¼‰
        
    *   éŸ³é¢‘å­—èŠ‚æ•°æµï¼Œä¾‹å¦‚ï¼šéº¦å…‹é£çš„å­—èŠ‚æ•°æ•°æ®
        
    *   wav.scpï¼ŒkaldiÂ æ ·å¼çš„Â wavÂ åˆ—è¡¨Â (`wav_idÂ \tÂ wav_path`),Â ä¾‹å¦‚:
        

```plaintext
asr_example1  ./audios/asr_example1.wav
asr_example2  ./audios/asr_example2.wav

```

åœ¨è¿™ç§è¾“å…¥Â 

*   éŸ³é¢‘é‡‡æ ·ç‚¹ï¼Œä¾‹å¦‚ï¼š`audio,Â rateÂ =Â soundfile.read("asr_example_zh.wav")`,Â æ•°æ®ç±»å‹ä¸ºÂ numpy.ndarrayã€‚æ”¯æŒbatchè¾“å…¥ï¼Œç±»å‹ä¸ºlistï¼šÂ `[audio_sample1,Â audio_sample2,Â ...,Â audio_sampleN]`
    
*   fbankè¾“å…¥ï¼Œæ”¯æŒç»„batchã€‚shapeä¸º\[batch,Â frames,Â dim\]ï¼Œç±»å‹ä¸ºtorch.Tensorï¼Œä¾‹å¦‚
    
*   `output_dir`:Â NoneÂ ï¼ˆé»˜è®¤ï¼‰ï¼Œå¦‚æœè®¾ç½®ï¼Œè¾“å‡ºç»“æœçš„è¾“å‡ºè·¯å¾„
    
*   `**kwargs`(dict):Â ä¸æ¨¡å‹ç›¸å…³çš„æ¨ç†å‚æ•°ï¼Œä¾‹å¦‚ï¼Œ`beam_size=10`ï¼Œ`decoding_ctc_weight=0.1`ã€‚
    

è¯¦ç»†æ–‡æ¡£é“¾æ¥ï¼š[https://github.com/modelscope/FunASR/blob/main/examples/README\_zh.md](https://github.com/modelscope/FunASR/blob/main/examples/README_zh.md)

# æ³¨å†Œè¡¨è¯¦è§£

## æ¨¡å‹èµ„æºç›®å½•

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/8oLl9y628rBNlapY/img/cab7f215-787f-4407-885a-14dc89ae9e02.png)

**æ¨¡å‹é“¾æ¥ä¸ºï¼š**[https://www.modelscope.cn/models/iic/SenseVoiceSmall/files](https://www.modelscope.cn/models/iic/SenseVoiceSmall/files)

**é…ç½®æ–‡ä»¶**ï¼šconfig.yaml

```yaml
encoder: SenseVoiceEncoderSmall
encoder_conf:
    output_size: 512
    attention_heads: 4
    linear_units: 2048
    num_blocks: 50
    tp_blocks: 20
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.1
    input_layer: pe
    pos_enc_class: SinusoidalPositionEncoder
    normalize_before: true
    kernel_size: 11
    sanm_shfit: 0
    selfattention_layer_type: sanm


model: SenseVoiceSmall
model_conf:
    length_normalized_loss: true
    sos: 1
    eos: 2
    ignore_id: -1

tokenizer: SentencepiecesTokenizer
tokenizer_conf:
  bpemodel: null
  unk_symbol: <unk>
  split_with_space: true

frontend: WavFrontend
frontend_conf:
    fs: 16000
    window: hamming
    n_mels: 80
    frame_length: 25
    frame_shift: 10
    lfr_m: 7
    lfr_n: 6
    cmvn_file: null


dataset: SenseVoiceCTCDataset
dataset_conf:
  index_ds: IndexDSJsonl
  batch_sampler: EspnetStyleBatchSampler
  data_split_num: 32
  batch_type: token
  batch_size: 14000
  max_token_length: 2000
  min_token_length: 60
  max_source_length: 2000
  min_source_length: 60
  max_target_length: 200
  min_target_length: 0
  shuffle: true
  num_workers: 4
  sos: ${model_conf.sos}
  eos: ${model_conf.eos}
  IndexDSJsonl: IndexDSJsonl
  retry: 20

train_conf:
  accum_grad: 1
  grad_clip: 5
  max_epoch: 20
  keep_nbest_models: 10
  avg_nbest_model: 10
  log_interval: 100
  resume: true
  validate_interval: 10000
  save_checkpoint_interval: 10000

optim: adamw
optim_conf:
  lr: 0.00002
scheduler: warmuplr
scheduler_conf:
  warmup_steps: 25000

```

**æ¨¡å‹å‚æ•°**ï¼šmodel.pt

**è·¯å¾„è§£æ**ï¼šconfiguration.json

```json
{
  "framework": "pytorch",
  "task" : "auto-speech-recognition",
  "model": {"type" : "funasr"},
  "pipeline": {"type":"funasr-pipeline"},
  "model_name_in_hub": {
    "ms":"", 
    "hf":""},
  "file_path_metas": {
    "init_param":"model.pt", 
    "config":"config.yaml",
    "tokenizer_conf": {"bpemodel": "chn_jpn_yue_eng_ko_spectok.bpe.model"},
    "frontend_conf":{"cmvn_file": "am.mvn"}}
}
```

## æ³¨å†Œè¡¨

### æŸ¥çœ‹æ³¨å†Œè¡¨

```plaintext
from funasr.register import tables

tables.print()
```

æ”¯æŒæŸ¥çœ‹æŒ‡å®šç±»å‹çš„æ³¨å†Œè¡¨ï¼š\`tables.print("model")\`

### æ³¨å†Œæ¨¡å‹

```python
from funasr.register import tables

@tables.register("model_classes", "SenseVoiceSmall")
class SenseVoiceSmall(nn.Module):
  def __init__(*args, **kwargs):
    ...

  def forward(
      self,
      **kwargs,
  ):  

  def inference(
      self,
      data_in,
      data_lengths=None,
      key: list = None,
      tokenizer=None,
      frontend=None,
      **kwargs,
  ):
    ...

```

åœ¨éœ€è¦æ³¨å†Œçš„ç±»åå‰åŠ ä¸Š `@tables.register("model_classes","SenseVoiceSmall")`ï¼Œå³å¯å®Œæˆæ³¨å†Œï¼Œç±»éœ€è¦å®ç°æœ‰ï¼š__init__ï¼Œforwardï¼Œinferenceæ–¹æ³•ã€‚

å®Œæ•´ä»£ç ï¼š[https://github.com/modelscope/FunASR/blob/main/funasr/models/sense\_voice/model.py#L443](https://github.com/modelscope/FunASR/blob/main/funasr/models/sense_voice/model.py#L443)

æ³¨å†Œå®Œæˆåï¼Œåœ¨config.yamlä¸­æŒ‡å®šæ–°æ³¨å†Œæ¨¡å‹ï¼Œå³å¯å®ç°å¯¹æ¨¡å‹çš„å®šä¹‰

```python
model: SenseVoiceSmall
model_conf:
  ...
```

## æ³¨å†ŒåŸåˆ™

*   Modelï¼šæ¨¡å‹ä¹‹é—´äº’ç›¸ç‹¬ç«‹ï¼Œæ¯ä¸€ä¸ªæ¨¡å‹ï¼Œéƒ½éœ€è¦åœ¨funasr/models/ä¸‹é¢æ–°å»ºä¸€ä¸ªæ¨¡å‹ç›®å½•ï¼Œä¸è¦é‡‡ç”¨ç±»çš„ç»§æ‰¿æ–¹æ³•ï¼ï¼ï¼ä¸è¦ä»å…¶ä»–æ¨¡å‹ç›®å½•ä¸­importï¼Œæ‰€æœ‰éœ€è¦ç”¨åˆ°çš„éƒ½å•ç‹¬æ”¾åˆ°è‡ªå·±çš„æ¨¡å‹ç›®å½•ä¸­ï¼ï¼ï¼ä¸è¦ä¿®æ”¹ç°æœ‰çš„æ¨¡å‹ä»£ç ï¼ï¼ï¼
    
*   datasetï¼Œfrontendï¼Œtokenizerï¼Œå¦‚æœèƒ½å¤ç”¨ç°æœ‰çš„ï¼Œç›´æ¥å¤ç”¨ï¼Œå¦‚æœä¸èƒ½å¤ç”¨ï¼Œè¯·æ³¨å†Œä¸€ä¸ªæ–°çš„ï¼Œå†ä¿®æ”¹ï¼Œä¸è¦ä¿®æ”¹åŸæ¥çš„ï¼ï¼ï¼
    

# ç‹¬ç«‹ä»“åº“

å¯ä»¥ä½œä¸ºç‹¬ç«‹ä»“åº“å­˜åœ¨ï¼Œç”¨äºä»£ç ä¿å¯†ï¼Œæˆ–è€…ç‹¬ç«‹å¼€æºã€‚åŸºäºæ³¨å†Œæœºåˆ¶ï¼Œæ— éœ€é›†æˆåˆ°funasrä¸­ï¼Œä½¿ç”¨funasrè¿›è¡Œæ¨ç†ï¼Œä¹Ÿå¯ä»¥ç›´æ¥è¿›è¡Œæ¨ç†ï¼ŒåŒæ ·æ”¯æŒfinetune

**ä½¿ç”¨AutoModelè¿›è¡Œæ¨ç†**

```python
from funasr import AutoModel

# trust_remote_codeï¼š`True` è¡¨ç¤º model ä»£ç å®ç°ä» `remote_code` å¤„åŠ è½½ï¼Œ`remote_code` æŒ‡å®š `model` å…·ä½“ä»£ç çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œå½“å‰ç›®å½•ä¸‹çš„ `model.py`ï¼‰ï¼Œæ”¯æŒç»å¯¹è·¯å¾„ä¸ç›¸å¯¹è·¯å¾„ï¼Œä»¥åŠç½‘ç»œ urlã€‚
model = AutoModel(
    model="iic/SenseVoiceSmall",
    trust_remote_code=True,  
    remote_code="./model.py", 
)
```

**ç›´æ¥è¿›è¡Œæ¨ç†**

```python
from model import SenseVoiceSmall

m, kwargs = SenseVoiceSmall.from_pretrained(model="iic/SenseVoiceSmall")
m.eval()

res = m.inference(
    data_in=f"{kwargs ['model_path']}/example/en.mp3",
    language="auto", # "zh", "en", "yue", "ja", "ko", "nospeech"
    use_itn=False,
    ban_emo_unk=False,
    **kwargs,
)

print(text)
```

å¾®è°ƒå‚è€ƒï¼š[https://github.com/FunAudioLLM/SenseVoice/blob/main/finetune.sh](https://github.com/FunAudioLLM/SenseVoice/blob/main/finetune.sh)