# FunASR-1.x.xÂ RegistrationÂ  New Model Tutorial

([ç®€ä½“ä¸­æ–‡](./Tables_zh.md)|English)

TheÂ originalÂ intentionÂ ofÂ theÂ funasr-1.x.xÂ versionÂ isÂ toÂ makeÂ modelÂ integrationÂ easier.Â TheÂ coreÂ featureÂ isÂ theÂ registryÂ andÂ AutoModel:

*   TheÂ introductionÂ ofÂ theÂ registryÂ enablesÂ theÂ developmentÂ ofÂ buildingÂ blocksÂ toÂ accessÂ theÂ model,Â compatibleÂ withÂ aÂ varietyÂ ofÂ tasks;
    
*   TheÂ newlyÂ designedÂ AutoModelÂ interfaceÂ unifiesÂ modelscope,Â huggingface,Â andÂ funasrÂ inferenceÂ andÂ trainingÂ interfaces,Â andÂ supportsÂ freeÂ downloadÂ ofÂ repositories;
    
*   SupportÂ modelÂ export,Â demo-levelÂ serviceÂ deployment,Â andÂ industrial-levelÂ multi-concurrentÂ serviceÂ deployment;
    
*   UnifyÂ academicÂ andÂ industrialÂ modelÂ inferenceÂ trainingÂ scripts;
    

# QuickÂ toÂ getÂ started

## AutoModelÂ usage

### SenseVoiceSmallÂ æ¨¡å‹

InputÂ anyÂ lengthÂ ofÂ voice,Â theÂ outputÂ asÂ theÂ voiceÂ contentÂ correspondingÂ toÂ theÂ text,Â theÂ textÂ hasÂ punctuationÂ brokenÂ sentences,Â supportÂ Chinese,Â English,Â Japanese,Â Guangdong,Â KoreanÂ andÂ 5Â ChineseÂ languages.Â \[Word-levelÂ timestampÂ andÂ speakerÂ identity\]Â willÂ beÂ supportedÂ later.

```python
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model = AutoModel(
    model="iic/SenseVoiceSmall",
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
)

res = model.generate(
    input=f"{model.model_path}/example/en.mp3",
    language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
    use_itn=True,
    batch_size_s=60,
)
text = rich_transcription_postprocess(res[0]["text"])
print(text) #ğŸ‘Senior staff, Priipal Doris Jackson, Wakefield faculty, and, of course, my fellow classmates.I am honored to have been chosen to speak before my classmates, as well as the students across America today.
```

## APIÂ documentation

#### DefinitionÂ ofÂ AutoModel

```plaintext
Model = AutoModel(model=[str], device=[str], ncpu=[int], output_dir=[str], batch_size= [int], hub=[str], **quargs)
```

*   `model`(str):Â [ModelÂ Warehouse](https://github.com/alibaba-damo-academy/FunASR/tree/main/model_zoo)TheÂ modelÂ nameÂ in,Â orÂ theÂ modelÂ pathÂ inÂ theÂ localÂ disk
    
*   `device`(str):Â `cuda:0`(DefaultÂ gpu0),Â usingÂ GPUÂ forÂ inference,Â specified.Â If`cpu`ThenÂ theÂ CPUÂ isÂ usedÂ forÂ inference
    
*   `ncpu`(int):Â `4`(Default),Â setÂ theÂ numberÂ ofÂ threadsÂ usedÂ forÂ CPUÂ internalÂ operationÂ parallelism
    
*   `output_dir`(str):Â `None`(Default)Â IfÂ set,Â theÂ outputÂ pathÂ ofÂ theÂ outputÂ result
    
*   `batch_size`(int):Â `1`(Default),Â batchÂ processingÂ duringÂ decoding,Â numberÂ ofÂ samples
    
*   `hub`(str)ï¼š`ms`(Default)Â toÂ downloadÂ theÂ modelÂ fromÂ modelscope.Â If`hf`ToÂ downloadÂ theÂ modelÂ fromÂ huggingface.
    
*   `**kwargs`(dict):Â AllÂ in`config.yaml`Parameters,Â whichÂ canÂ beÂ specifiedÂ directlyÂ here,Â forÂ example,Â theÂ maximumÂ cutÂ lengthÂ inÂ theÂ vadÂ model.`max_single_segment_time=6000`(Milliseconds).
    

#### AutoModelÂ reasoning

```plaintext
Res = model.generate(input=[str], output_dir=[str])
```

*   *   wavÂ fileÂ path,Â forÂ example:Â asr\_example.wav
        
    *   pcmÂ fileÂ path,Â forÂ example:Â asr\_example.pcm,Â youÂ needÂ toÂ specifyÂ theÂ audioÂ samplingÂ rateÂ fsÂ (defaultÂ isÂ 16000)
        
    *   AudioÂ byteÂ stream,Â forÂ example:Â microphoneÂ byteÂ data
        
    *   wav.scp,kaldi-styleÂ wavÂ listÂ (`wav_idÂ \tÂ wav_path`),Â forÂ example:
        

```plaintext
Asr_example1./audios/asr_example1.wav
Asr_example2./audios/asr_example2.wav

```

InÂ thisÂ input

*   AudioÂ samplingÂ points,Â forÂ example:`audio,Â rateÂ =Â soundfile.read("asr_example_zh.wav")`IsÂ numpy.ndarray.Â batchÂ inputÂ isÂ supported.Â TheÂ typeÂ isÂ list:`[audio_sample1,Â audio_sample2,Â ...,Â audio_sampleN]`
    
*   fbankÂ input,Â supportÂ groupÂ batch.Â shapeÂ isÂ \[batch,Â frames,Â dim\],Â typeÂ isÂ torch.Tensor,Â forÂ example
    
*   `output_dir`:Â NoneÂ (default),Â ifÂ set,Â theÂ outputÂ pathÂ ofÂ theÂ outputÂ result
    
*   `**kwargs`(dict):Â Model-relatedÂ inferenceÂ parameters,Â e.g,`beam_size=10`,`decoding_ctc_weight=0.1`.
    

DetailedÂ documentationÂ link:[https://github.com/modelscope/FunASR/blob/main/examples/README\_zh.md](https://github.com/modelscope/FunASR/blob/main/examples/README_zh.md)

# RegistryÂ Details

TakeÂ theÂ SenseVoiceSmallÂ modelÂ asÂ anÂ example,Â explainÂ howÂ toÂ registerÂ aÂ newÂ model,Â modelÂ link:

**modelscopeï¼š**[https://www.modelscope.cn/models/iic/SenseVoiceSmall/files](https://www.modelscope.cn/models/iic/SenseVoiceSmall/files)

**huggingfaceï¼š**[https://huggingface.co/FunAudioLLM/SenseVoiceSmall](https://huggingface.co/FunAudioLLM/SenseVoiceSmall)

## ModelÂ ResourceÂ Catalog

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/8oLl9y628rBNlapY/img/cab7f215-787f-4407-885a-14dc89ae9e02.png)

ConfigurationÂ File:Â config.yaml

```yaml
encoder: SenseVoiceEncoderSmall
encoder_conf:
    output_size: 512
    attention_heads: 4
    linear_units: 2048
    num_blocks: 50
    tp_blocks: 20
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    attention_dropout_rate: 0.1
    input_layer: pe
    pos_enc_class: SinusoidalPositionEncoder
    normalize_before: true
    kernel_size: 11
    sanm_shfit: 0
    selfattention_layer_type: sanm


model: SenseVoiceSmall
model_conf:
    length_normalized_loss: true
    sos: 1
    eos: 2
    ignore_id: -1

tokenizer: SentencepiecesTokenizer
tokenizer_conf:
  bpemodel: null
  unk_symbol: <unk>
  split_with_space: true

frontend: WavFrontend
frontend_conf:
    fs: 16000
    window: hamming
    n_mels: 80
    frame_length: 25
    frame_shift: 10
    lfr_m: 7
    lfr_n: 6
    cmvn_file: null


dataset: SenseVoiceCTCDataset
dataset_conf:
  index_ds: IndexDSJsonl
  batch_sampler: EspnetStyleBatchSampler
  data_split_num: 32
  batch_type: token
  batch_size: 14000
  max_token_length: 2000
  min_token_length: 60
  max_source_length: 2000
  min_source_length: 60
  max_target_length: 200
  min_target_length: 0
  shuffle: true
  num_workers: 4
  sos: ${model_conf.sos}
  eos: ${model_conf.eos}
  IndexDSJsonl: IndexDSJsonl
  retry: 20

train_conf:
  accum_grad: 1
  grad_clip: 5
  max_epoch: 20
  keep_nbest_models: 10
  avg_nbest_model: 10
  log_interval: 100
  resume: true
  validate_interval: 10000
  save_checkpoint_interval: 10000

optim: adamw
optim_conf:
  lr: 0.00002
Scheduler: warmuplr
Scheduler_conf:
Warmup_steps: 25000

```

ModelÂ parameters:Â model.pt

PathÂ resolution:Â configuration.jsonÂ (notÂ required)

```json
{
  "framework": "pytorch",
  "task" : "auto-speech-recognition",
  "model": {"type" : "funasr"},
  "pipeline": {"type":"funasr-pipeline"},
  "model_name_in_hub": {
    "ms":"", 
    "hf":""},
  "file_path_metas": {
    "init_param":"model.pt", 
    "config":"config.yaml",
    "tokenizer_conf": {"bpemodel": "chn_jpn_yue_eng_ko_spectok.bpe.model"},
    "frontend_conf":{"cmvn_file": "am.mvn"}}
}
```

TheÂ functionÂ ofÂ configuration.jsonÂ isÂ toÂ addÂ theÂ modelÂ rootÂ directoryÂ toÂ theÂ itemÂ inÂ file\_path\_metas,Â soÂ thatÂ theÂ pathÂ canÂ beÂ correctlyÂ parsed.Â ForÂ example,Â assumeÂ thatÂ theÂ modelÂ rootÂ directoryÂ is:/home/zhifu.gzf/init\_model/SenseVoiceSmall,TheÂ relevantÂ pathÂ inÂ config.yamlÂ inÂ theÂ directoryÂ isÂ replacedÂ withÂ theÂ correctÂ pathÂ (ignoringÂ irrelevantÂ configuration):

```yaml
init_param: /home/zhifu.gz F/init_model/sensevoicemail Mall/model.pt

tokenizer_conf:
  bpemodel: /home/Zhifu.gzf/init_model/SenseVoiceSmall/chn_jpn_yue_eng_ko_spectok.bpe.model

frontend_conf:
    cmvn_file: /home/zhifu.Gzf/init_model/SenseVoiceSmall/am.mvn
```

## Registry

![image](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/a/pDaAnLxn5IX2w9Y1/73da157edae94d78b68c8d30c8e085eb0521.png)

### ViewÂ Registry

```plaintext
from funasr.register import tables

tables.print()
```

SupportÂ toÂ viewÂ theÂ specifiedÂ typeÂ ofÂ Registry:Â 'tables.print("model")',Â currentlyÂ funasrÂ hasÂ registeredÂ modelÂ asÂ shownÂ inÂ theÂ figureÂ above.Â TheÂ followingÂ categoriesÂ areÂ currentlyÂ predefined:

```python
    model_classes = {}
    frontend_classes = {}
    specaug_classes = {}
    normalize_classes = {}
    encoder_classes = {}
    decoder_classes = {}
    joint_network_classes = {}
    predictor_classes = {}
    stride_conv_classes = {}
    tokenizer_classes = {}
    dataloader_classes = {}
    batch_sampler_classes = {}
    dataset_classes = {}
    index_ds_classes = {}
```

### RegistrationÂ Model

```python
from funasr.register import tables

@tables.register("model_classes", "SenseVoiceSmall")
class SenseVoiceSmall(nn.Module):
  def __init__(*args, **kwargs):
    ...

  def forward(
      self,
      **kwargs,
  ):  

  def inference(
      self,
      data_in,
      data_lengths=None,
      key: list = None,
      tokenizer=None,
      frontend=None,
      **kwargs,
  ):
    ...

```

AddÂ @Â tables.register("model\_classes",Â "SenseVoiceSmall")Â beforeÂ theÂ nameÂ ofÂ theÂ classÂ toÂ beÂ registered.Â TheÂ classÂ needsÂ toÂ implementÂ theÂ followingÂ methods:\_\_initÂ \_\_,Â forward,Â andÂ inference.

registerÂ Usage:

```python
@ tables.register("registration classification", "registration name")
```

AmongÂ them,Â "registrationÂ classification"Â canÂ beÂ aÂ predefinedÂ classificationÂ (seeÂ theÂ figureÂ above).Â IfÂ itÂ isÂ aÂ newÂ classificationÂ definedÂ byÂ oneself,Â theÂ newÂ classificationÂ willÂ beÂ automaticallyÂ writtenÂ intoÂ theÂ registryÂ classification.Â "registrationÂ name"Â meansÂ theÂ nameÂ youÂ wantÂ toÂ registerÂ andÂ canÂ beÂ usedÂ directlyÂ inÂ theÂ future.

FullÂ code:[https://github.com/modelscope/FunASR/blob/main/funasr/models/sense\_voice/model.py#L443](https://github.com/modelscope/FunASR/blob/main/funasr/models/sense_voice/model.py#L443)

AfterÂ theÂ registrationÂ isÂ complete,Â specifyÂ theÂ newÂ registrationÂ modelÂ inÂ config.yamlÂ toÂ defineÂ theÂ model.

```python
model: SenseVoiceSmall
model_conf:
  ...
```

### RegistrationÂ failed

IfÂ theÂ registrationÂ modelÂ orÂ methodÂ isÂ notÂ found,Â assertÂ model\_classÂ isÂ notÂ None,Â f'{kwargs\["model"\]}Â isÂ notÂ registeredÂ '.Â TheÂ principleÂ ofÂ modelÂ registrationÂ isÂ toÂ importÂ theÂ modelÂ file,YouÂ canÂ viewÂ theÂ specificÂ reasonÂ forÂ theÂ registrationÂ failureÂ throughÂ import.Â ForÂ example,Â theÂ precedingÂ modelÂ fileÂ isÂ funasr/models/sense\_voice/model.py:

```python
from funasr.models.sense_voice.model import *
```

## PrinciplesÂ ofÂ Registration

*   Model:Â modelsÂ areÂ independentÂ ofÂ eachÂ other.Â EachÂ ModelÂ needsÂ toÂ createÂ aÂ newÂ ModelÂ directoryÂ underÂ funasr/models/.Â DoÂ notÂ useÂ classÂ inheritanceÂ method!!!Â DoÂ notÂ importÂ fromÂ otherÂ modelÂ directories,Â andÂ putÂ everythingÂ youÂ needÂ intoÂ yourÂ ownÂ modelÂ directory!!!Â DoÂ notÂ modifyÂ theÂ existingÂ modelÂ code!!!
    
*   dataset,frontend,tokenizer,Â ifÂ youÂ canÂ reuseÂ theÂ existingÂ one,Â reuseÂ itÂ directly,Â ifÂ youÂ cannotÂ reuseÂ it,Â pleaseÂ registerÂ aÂ newÂ one,Â modifyÂ itÂ again,Â andÂ doÂ notÂ modifyÂ theÂ originalÂ one!!!
    

# IndependentÂ warehouse

ItÂ canÂ existÂ asÂ aÂ stand-aloneÂ repositoryÂ forÂ codeÂ secrecy,Â orÂ asÂ aÂ stand-aloneÂ openÂ source.Â BasedÂ onÂ theÂ registrationÂ mechanism,Â youÂ doÂ notÂ needÂ toÂ integrateÂ itÂ intoÂ funasr.Â YouÂ canÂ alsoÂ useÂ funasrÂ forÂ inference,Â andÂ youÂ canÂ alsoÂ directlyÂ performÂ inference.Â finetuneÂ isÂ alsoÂ supported.

**UsingÂ AutoModelÂ forÂ inference**

```python
from funasr import AutoModel

# trust_remote_code:'True' means that the model code implementation is loaded from 'remote_code', 'remote_code' specifies the location of the 'model' specific code (for example,'model.py') in the current directory, supports absolute and relative paths, and network url.
model = AutoModel (
model="iic/SenseVoiceSmall ",
trust_remote_code=True
remote_code = "./model.py", 
)
```

**DirectÂ inference**

```python
from model import SenseVoiceSmall

m, kwargs = SenseVoiceSmall.from_pretrained(model="iic/SenseVoiceSmall")
m.eval()

res = m.inference(
    data_in=f"{kwargs ['model_path']}/example/en.mp3",
    language="auto", # "zh", "en", "yue", "ja", "ko", "nospeech"
    use_itn=False,
    ban_emo_unk=False,
    **kwargs,
)

print(text)
```

TrimÂ reference:[https://github.com/FunAudioLLM/SenseVoice/blob/main/finetune.sh](https://github.com/FunAudioLLM/SenseVoice/blob/main/finetune.sh)